{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833e8cf-3559-4cf7-98ee-679a54f4ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import src.preprocess\n",
    "import src.io\n",
    "\n",
    "importlib.reload(src.preprocess)\n",
    "importlib.reload(src.io) \n",
    "\n",
    "DATA_ROOT = \"gs://bird-clef-kimmo/data\"\n",
    "TRAIN_SHORT_AUDIO_DATA = f\"{DATA_ROOT}/train_short_audio\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "SR = 32000\n",
    "SPLIT_SECS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37971931-5f80-4559-9df9-ce273d49ac48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03a9ae-3f90-4633-9a9d-27d8536bbca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_ds = src.io.train_metadata_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1434a16-e3c8-42ca-bf3c-c67eb252f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_metadata_ds.take(1):\n",
    "    pprint(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da74abe-79d5-4384-a801-cd298136a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = src.io.read_classes()\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416b870-b5bd-4726-b43e-5f39f25400ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(url) -> tf.Tensor:\n",
    "    logging.debug(f\"Reading file: {url}\")\n",
    "    return tf.squeeze(tfio.audio.AudioIOTensor(url).to_tensor(), axis=1)  # remove channel axis\n",
    "\n",
    "def add_audio(row):\n",
    "    filename = row[\"filename\"]\n",
    "    primary_label = row[\"primary_label\"]\n",
    "    file_url = TRAIN_SHORT_AUDIO_DATA + \"/\" + primary_label + \"/\" + filename\n",
    "    [audio,] = tf.py_function(read_file, [file_url], [tf.float32])\n",
    "    return {**row, \"file_url\": file_url, \"audio\": audio}\n",
    "\n",
    "for row in train_metadata_ds.map(add_audio).take(1):\n",
    "    pprint(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad760cf-2298-4c86-8c51-8bd89acec1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 3\n",
    "n = rows * cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n",
    "\n",
    "for i, sample in enumerate(train_metadata_ds.map(add_audio).take(n)):\n",
    "    r = i // cols\n",
    "    c = i % cols\n",
    "    ax = axes[r][c]\n",
    "    audio = sample[\"audio\"].numpy()\n",
    "    label = sample[\"scientific_name\"].numpy().decode()\n",
    "    ax.plot(np.arange(len(audio)) / SR, audio)\n",
    "    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n",
    "    # label = label.numpy().decode('utf-8')\n",
    "    ax.set_title(label)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "Audio(audio, rate=SR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac74873-f10f-49cc-84c8-311df09ea405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_audio(audio):\n",
    "    length = audio.shape[-1]\n",
    "    \n",
    "    n_splits = length // (SR * SPLIT_SECS)\n",
    "        \n",
    "    splits = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        start = i * SR*SPLIT_SECS\n",
    "        end = (i+1) * SR*SPLIT_SECS\n",
    "        split = audio[start:end]\n",
    "        splits.append(split)\n",
    "    \n",
    "    return tf.convert_to_tensor(splits)\n",
    "\n",
    "def split_to_segments(rows):\n",
    "    audio = rows[\"audio\"]\n",
    "    \n",
    "    splits = tf.py_function(split_audio, [audio], tf.float32)\n",
    "    \n",
    "    original = tf.data.Dataset.from_tensors(rows).repeat()\n",
    "    splits_ = tf.data.Dataset.from_tensor_slices({\"segment\": splits})\n",
    "    \n",
    "    def combine_keys(a, b):\n",
    "        return {**a, **b}\n",
    "    \n",
    "    zipped = tf.data.Dataset.zip((original, splits_)).map(combine_keys)\n",
    "    return zipped\n",
    "\n",
    "def add_label(sample):\n",
    "    label = src.io.primary_label_to_tensor(sample[\"primary_label\"], CLASSES)\n",
    "    return sample, label\n",
    "\n",
    "def drop_keys(*keys):\n",
    "    \n",
    "    def drop(rows):\n",
    "        rows = rows.copy()\n",
    "        for key in keys:\n",
    "            rows.pop(key)\n",
    "        return rows\n",
    "\n",
    "    return drop\n",
    "\n",
    "short_audio_ds = train_metadata_ds.map(add_audio).flat_map(split_to_segments).map(drop_keys(\"filename\")).map(add_label)\n",
    "\n",
    "for sample, label in short_audio_ds.take(3):\n",
    "    pprint(sample)\n",
    "    pprint(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e9a769-274c-4cb5-ab8e-f41c8586e2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
